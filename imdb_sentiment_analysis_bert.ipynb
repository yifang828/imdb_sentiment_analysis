{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取資料\n",
    "看前五筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./movie_data.csv', encoding='utf-8')\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看資料筆數和欄位數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-processing\n",
    "1. to lowercase\n",
    "2. remove url and html tag\n",
    "3. remove puctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# to lowercase\n",
    "df['review'] = df['review'].str.lower()\n",
    "\n",
    "# remove url and html tag\n",
    "df['review'] = df['review'].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove puctuation\n",
    "df['review'] = df['review'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+', '', x) for x in str(x).split()]))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分資料集 train, val, test，其中train:test=7:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], train_size=0.7, test_size=0.3)\n",
    "# # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.7, test_size=0.3)\n",
    "# print(x_train.head(5))\n",
    "# print(x_test.head(5))\n",
    "# print(y_train.head(5))\n",
    "# print(y_test.head(5))\n",
    "\n",
    "# dict = {'review':x_test, 'sentiment':y_test}\n",
    "# df = pd.DataFrame(dict)\n",
    "# df.to_csv('imdb_test.csv', header=True, index=False)\n",
    "\n",
    "# # dict = {'review':x_val, 'sentiment':y_val}\n",
    "# # df = pd.DataFrame(dict)\n",
    "# # df.to_csv('imdb_val.csv', header=True, index=False)\n",
    "\n",
    "# dict = {'review':x_train, 'sentiment':y_train}\n",
    "# df = pd.DataFrame(dict)\n",
    "# df.to_csv('imdb_train.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('imdb_test.csv', encoding='utf-8')\n",
    "print(test.shape)\n",
    "print(test['sentiment'].value_counts())\n",
    "\n",
    "train = pd.read_csv('imdb_train.csv', encoding='utf-8')\n",
    "print(train.shape)\n",
    "print(train['sentiment'].value_counts())\n",
    "\n",
    "# val = pd.read_csv('imdb_val.csv', encoding='utf-8')\n",
    "# print(val.shape)\n",
    "# print(val['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieComment(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train','val', 'test']\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.df = pd.read_csv('imdb_train_bert.csv')\n",
    "        elif self.mode == 'val':\n",
    "            self.df = pd.read_csv('imdb_val_bert.csv')\n",
    "        elif self.mode == 'test':\n",
    "            self.df = pd.read_csv('imdb_test.csv')\n",
    "        # required_label\n",
    "        \n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {0: 0, 1: 1}\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 0]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        label_id = self.label_map[label]\n",
    "        label_tensor = torch.tensor(label_id)\n",
    "        word_pieces = ['[CLS]']\n",
    "        tokens = self.tokenizer.tokenize(text[:510])\n",
    "        word_pieces += tokens + ['[SEP]']\n",
    "        len_token = len(word_pieces)\n",
    "\n",
    "        # 將整個token序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        segment_tensor = torch.tensor([0]*len_token, dtype = torch.long)\n",
    "        return(tokens_tensor, segment_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification acc: 0.50425\n",
      "EPOCH:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:23<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 546.777, train_acc: 0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] val_acc: 0.812\n",
      "EPOCH:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:23<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] train_loss: 353.667, train_acc: 0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] val_acc: 0.854\n",
      "EPOCH:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:22<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] train_loss: 308.078, train_acc: 0.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] val_acc: 0.868\n",
      "EPOCH:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:22<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] train_loss: 283.352, train_acc: 0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] val_acc: 0.880\n",
      "EPOCH:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:21<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] train_loss: 266.732, train_acc: 0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] val_acc: 0.889\n",
      "EPOCH:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:22<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] train_loss: 254.413, train_acc: 0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] val_acc: 0.895\n",
      "EPOCH:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:22<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] train_loss: 248.416, train_acc: 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] val_acc: 0.898\n",
      "EPOCH:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:24<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] train_loss: 242.805, train_acc: 0.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] val_acc: 0.901\n",
      "EPOCH:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:22<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] train_loss: 239.154, train_acc: 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] val_acc: 0.902\n",
      "EPOCH:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [03:22<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] train_loss: 239.814, train_acc: 0.890\n",
      "[epoch 10] val_acc: 0.902\n",
      "execute_time:  3768.1931476593018\n",
      "======================================\n",
      "test accuracy:  0.8690666666666667\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "PRETRAINED_TOKENIZER = 'bert-base-uncased'\n",
    "NUM_LABELS = 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 10\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    # 測試集有 labels\n",
    "    if len(samples[0])==3:\n",
    "        if samples[0][2] is not None:\n",
    "            label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "# 利用collate_fn將list of samples 合併成一個 mini-batch是關鍵\n",
    "trainset = MovieComment('train', tokenizer=tokenizer)\n",
    "trainloader =  DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "valset = MovieComment('val', tokenizer=tokenizer)\n",
    "valloader =  DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 遍尋整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有tensor 移到GPU上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda\") for t in data if t is not None]\n",
    "            # 前3個tensors分別為tokens, segments, masks，建議再將這些tensors丟入model時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "\n",
    "            # 當前batch記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "\n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "# 讓模型跑在GPU上並取得訓練集的分類準確率\n",
    "model = model.to(device)\n",
    "# bert fine-tune 前的準確率\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)\n",
    "\n",
    "# 計時開始\n",
    "start_time = time.time()\n",
    "# 使用Adam Optim更新整個分類模型的參數\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-2)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_training_steps=EPOCH * len(trainloader), num_warmup_steps=len(trainloader))\n",
    "for epoch in range(EPOCH):\n",
    "    print('EPOCH: ', epoch + 1)\n",
    "    # 訓練模型\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainloader):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, \\\n",
    "        labels = [t.to(device) for t in data]\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors,\\\n",
    "            attention_mask=masks_tensors, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # 紀錄當前batch loss\n",
    "        running_loss += loss.item()\n",
    "    #計算分類準確率\n",
    "    _, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    print('[epoch %d] train_loss: %.3f, train_acc: %.3f' %(epoch+1, running_loss, train_acc))\n",
    "    model.eval()\n",
    "    _, val_acc = get_predictions(model, valloader, compute_acc=True)\n",
    "    print('[epoch %d] val_acc: %.3f' %(epoch+1, val_acc))\n",
    "\n",
    "end_time = time.time()\n",
    "print('execute_time: ', str(end_time-start_time))\n",
    "\n",
    "### predict testset\n",
    "testset = MovieComment('test', tokenizer=tokenizer)\n",
    "y_true = testset.df.iloc[:,1].values\n",
    "testloader =  DataLoader(testset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "prediction = get_predictions(model, testloader, compute_acc=False)\n",
    "print(\"======================================\")\n",
    "print('test accuracy: ',accuracy_score(y_true, prediction.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a9e08d3d7ccfa255d62ba1e587172a6adb964258b9aeb02178b449ad672450b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
